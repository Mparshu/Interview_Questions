The Problem: You are given a DataFrame df with student data, which includes their names and cumulative GPAs. The DataFrame schema includes columns like student_name and cumulative_gpa.
Tasks:
1.Find the maximum, minimum, and average cumulative GPA from the DataFrame.
2.Determine the student with the highest GPA and the student with the lowest GPA.
3.Count the number of students whose GPA matches the rounded average GPA.
4.Create a new DataFrame with a schema containing the names of the students with the highest and lowest GPAs, as well as the count of students with the average GPA.
5.Display this new DataFrame.

from pyspark.sql.functions import col, max, min, avg, round

# Finding the max, min, and average GPA
max_gpa = df.select(max(col("cumulative_gpa"))).collect()[0][0]
min_gpa = df.select(min(col("cumulative_gpa"))).collect()[0][0]
avg_gpa = df.select(round(avg(col("cumulative_gpa")), 2)).collect()[0][0]

print(max_gpa, min_gpa, avg_gpa)

# Determining the students with the highest and lowest GPAs
max_student = df.filter(col("cumulative_gpa") == max_gpa).select("student_name").collect()[0][0]
min_student = df.filter(col("cumulative_gpa") == min_gpa).select("student_name").collect()[0][0]

# Counting the number of students with the rounded average GPA
avg_count = df.filter(col("cumulative_gpa") == avg_gpa).count()

# Creating a new DataFrame with the required schema
schema = ("top_name string, last_name string, avg_count int")
data = [[max_student, min_student, avg_count]]
final_df = spark.createDataFrame(data, schema)

# Displaying the new DataFrame
display(final_df)

bike_sharing_data = spark.read.format('csv') \
                    .option("Inferschema", "True") \
                    .option("header", "True") \
                    .option("sep",",") \
                    .load("/home/data/bike_sharing_data.csv")


bike_sharing_data.createOrReplaceTempView('bike_sharing_data)


1. How do you optimize the performance of your PySpark jobs?
2. Can you discuss the techniques you use to handle skewed data in PySpark?
3. Can you describe how you've applied Spark optimization techniques in your previous roles?
4. Could you provide an overview of your experience with PySpark and big data processing?
5. Can you explain the basic architecture of PySpark in detail?
6. What are the differences between a DataFrame and an RDD in PySpark, and in what scenarios would you use an RDD?
7. How does PySpark relate to Apache Spark, and what advantages does it offer for distributed data processing?
8. Can you explain the concepts of transformations and actions in PySpark?
9. Could you provide examples of PySpark DataFrame operations that you frequently use?
10. How does data serialization work in PySpark?
11. Can you discuss the significance of choosing the right compression codec for your PySpark applications?
12. How do you handle missing or null values in PySpark?
13. Are there any specific strategies or functions you prefer for handling missing data in PySpark?
14. What are some common challenges you've encountered while working with PySpark, and how have you overcome them?
15. How do you ensure data quality and integrity when processing large datasets with PySpark?
16. Can you share an example of a complex PySpark project you've worked on and the results you achieved?


Difference between pyspark df and pandas df.
Spark DataFrame
Spark DataFrame supports parallelization. 
Spark DataFrame has Multiple Nodes.
It follows Lazy Execution which means that a task is not executed until an action is performed.
Spark DataFrame is Immutable.
Complex operations are difficult to perform as compared to Pandas DataFrame.
Spark DataFrame is distributed and hence processing in the Spark DataFrame is faster for a large amount of data.
sparkDataFrame.count() returns the number of rows.
Spark DataFrames are excellent for building a scalable application.
Spark DataFrame assures fault tolerance.

Pandas DataFrame
Pandas DataFrame does not support parallelization. 
Pandas DataFrame has a Single  Node.
It follows Eager Execution, which means task is executed immediately.
Pandas DataFrame is Mutable.
Complex operations are easier to perform as compared to Spark DataFrame.
Pandas DataFrame is not distributed and hence processing in the Pandas DataFrame will be slower for a large amount of data.
pandasDataFrame.count() returns the number of non NA/null observations for each column.
Pandas DataFrames canâ€™t be used to build a scalable application.
Pandas DataFrame does not assure fault tolerance. We need to implement our own framework to assure it.
